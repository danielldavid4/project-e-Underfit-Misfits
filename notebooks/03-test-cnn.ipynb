{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project E – Evaluation of Trained CoordConv CNN\n",
    "\n",
    "This notebook loads the trained CoordConv-based CNN from disk and evaluates its performance on a separate test dataset. It computes classification metrics (accuracy, precision, recall, F1-score) and localization quality via Intersection over Union (IoU). When labels are available, it also generates visualizations comparing predicted and ground-truth bounding boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T21:20:16.230940Z",
     "iopub.status.busy": "2025-12-01T21:20:16.230197Z",
     "iopub.status.idle": "2025-12-01T21:20:16.238190Z",
     "shell.execute_reply": "2025-12-01T21:20:16.237358Z",
     "shell.execute_reply.started": "2025-12-01T21:20:16.230914Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class AddCoords(layers.Layer):\n",
    "    def call(self, input_tensor):\n",
    "        batch_size_tensor = tf.shape(input_tensor)[0]\n",
    "        x_dim = tf.shape(input_tensor)[2]\n",
    "        y_dim = tf.shape(input_tensor)[1]\n",
    "        \n",
    "        # xx_channel encodes column indices\n",
    "        xx_ones = tf.ones([batch_size_tensor, x_dim], dtype='int32')\n",
    "        xx_ones = tf.expand_dims(xx_ones, -1)\n",
    "        xx_range = tf.tile(tf.expand_dims(tf.range(y_dim), 0), [batch_size_tensor, 1])\n",
    "        xx_range = tf.expand_dims(xx_range, 1)\n",
    "        xx_channel = tf.matmul(xx_ones, xx_range)\n",
    "        xx_channel = tf.expand_dims(xx_channel, -1)\n",
    "        xx_channel = tf.cast(xx_channel, 'float32') / (tf.cast(x_dim, 'float32') - 1)\n",
    "\n",
    "        # yy_channel encodes row indices\n",
    "        yy_ones = tf.ones([batch_size_tensor, y_dim], dtype='int32')\n",
    "        yy_ones = tf.expand_dims(yy_ones, 1)\n",
    "        yy_range = tf.tile(tf.expand_dims(tf.range(x_dim), 0), [batch_size_tensor, 1])\n",
    "        yy_range = tf.expand_dims(yy_range, -1)\n",
    "        yy_channel = tf.matmul(yy_range, yy_ones)\n",
    "        yy_channel = tf.expand_dims(yy_channel, -1)\n",
    "        yy_channel = tf.cast(yy_channel, 'float32') / (tf.cast(y_dim, 'float32') - 1)\n",
    "\n",
    "        # Concatenate original features with coordinate channels\n",
    "        return tf.concat([input_tensor, xx_channel, yy_channel], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the trained model\n",
    "\n",
    "We load the trained CoordConv CNN from disk. The `AddCoords` custom layer must be provided through `custom_objects` so that Keras can reconstruct the model correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this path to wherever you saved the final or best model in 02_train_cnn\n",
    "MODEL_PATH = '/kaggle/working/final_model_coord.keras'  # or '../models/best_model_coord.keras'\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={'AddCoords': AddCoords}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intersection over Union (IoU) metric\n",
    "\n",
    "Here we define a helper function that computes the IoU between ground-truth and predicted bounding boxes given in normalized `[x, y, w, h]` form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(box_true, box_pred):\n",
    "    x1_t, y1_t, w_t, h_t = box_true[:, 0], box_true[:, 1], box_true[:, 2], box_true[:, 3]\n",
    "    x2_t, y2_t = x1_t + w_t, y1_t + h_t\n",
    "    \n",
    "    x1_p, y1_p, w_p, h_p = box_pred[:, 0], box_pred[:, 1], box_pred[:, 2], box_pred[:, 3]\n",
    "    x2_p, y2_p = x1_p + w_p, y1_p + h_p\n",
    "    \n",
    "    # Intersection rectangle\n",
    "    x1_i = np.maximum(x1_t, x1_p)\n",
    "    y1_i = np.maximum(y1_t, y1_p)\n",
    "    x2_i = np.minimum(x2_t, x2_p)\n",
    "    y2_i = np.minimum(y2_t, y2_p)\n",
    "    \n",
    "    w_i = np.maximum(0, x2_i - x1_i)\n",
    "    h_i = np.maximum(0, y2_i - y1_i)\n",
    "    inter_area = w_i * h_i\n",
    "    \n",
    "    # Union area\n",
    "    box_t_area = w_t * h_t\n",
    "    box_p_area = w_p * h_p\n",
    "    union_area = box_t_area + box_p_area - inter_area\n",
    "    \n",
    "    return inter_area / (union_area + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and preprocess the test dataset\n",
    "\n",
    "The professor will provide a blind test dataset with `.npy` files for the input frames and (possibly) the labels:\n",
    "\n",
    "- `DATA_PATH`  – video frames\n",
    "- `LABEL_PATH` – class IDs and bounding boxes (only if labels are provided)\n",
    "\n",
    "We reshape the videos into frame-level samples and normalize pixel values to match the training preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update these when your professor gives you the test file paths\n",
    "DATA_PATH = '/kaggle/input/blindset/test_data_projectE.npy'\n",
    "LABEL_PATH = '/kaggle/input/blindset/test_labels_projectE.npy'  # if labels are provided\n",
    "\n",
    "X_raw = np.load(DATA_PATH)\n",
    "\n",
    "# Flatten videos → frames and normalize\n",
    "X_test = X_raw.reshape(-1, 100, 100, 3).astype('float32') / 255.0\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# If labels are provided (for accuracy & IoU):\n",
    "labels_available = True  # set False if the blind set has no labels\n",
    "\n",
    "if labels_available:\n",
    "    y_raw = np.load(LABEL_PATH, allow_pickle=True)\n",
    "    y_flat = y_raw.reshape(-1, 5).astype('float32')\n",
    "    y_true_cls = (y_flat[:, 0] - 1).astype(int)\n",
    "    y_true_box = y_flat[:, 1:] / 100.0  # normalize boxes to [0, 1]\n",
    "    print(\"y_true_cls shape:\", y_true_cls.shape)\n",
    "    print(\"y_true_box shape:\", y_true_box.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict classes and bounding boxes\n",
    "\n",
    "We feed the test frames into the trained model to obtain:\n",
    "\n",
    "- `pred_probs` – class probabilities from the softmax head.\n",
    "- `pred_boxes` – predicted normalized bounding boxes from the regression head.\n",
    "\n",
    "We also apply the same confidence-based UNKNOWN mechanism used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting on test set...\")\n",
    "pred_probs, pred_boxes = model.predict(X_test, batch_size=64)\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "\n",
    "final_preds = []\n",
    "for p in pred_probs:\n",
    "    if np.max(p) < CONFIDENCE_THRESHOLD:\n",
    "        final_preds.append(5)  # 5 = UNKNOWN\n",
    "    else:\n",
    "        final_preds.append(np.argmax(p))\n",
    "final_preds = np.array(final_preds)\n",
    "\n",
    "print(\"Example predicted box:\", pred_boxes[0])\n",
    "print(\"Max predicted box value:\", pred_boxes.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification and IoU metrics (when labels are available)\n",
    "\n",
    "If the blind test set includes labels, we can compute:\n",
    "\n",
    "- Overall **classification accuracy**  \n",
    "- **Per-class precision, recall, and F1-score** via `classification_report`  \n",
    "- **Mean IoU** between predicted and ground-truth boxes on valid frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if labels_available:\n",
    "    # Only evaluate where ground-truth boxes are valid (width > 0)\n",
    "    valid_mask = (y_true_box[:, 2] > 0)\n",
    "    y_true_box_valid = y_true_box[valid_mask]\n",
    "    pred_boxes_valid = pred_boxes[valid_mask]\n",
    "    final_preds_valid = final_preds[valid_mask]\n",
    "    y_true_cls_valid = y_true_cls[valid_mask]\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true_cls_valid, final_preds_valid)\n",
    "\n",
    "    # IoU\n",
    "    ious = compute_iou(y_true_box_valid, pred_boxes_valid)\n",
    "    correct_mask = (final_preds_valid == y_true_cls_valid)\n",
    "    mean_iou = np.mean(ious[correct_mask]) if np.sum(correct_mask) > 0 else 0.0\n",
    "\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Test Mean IoU (correctly classified): {mean_iou:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    label_names = ['Ball', 'Mug', 'Pen', 'Spoon', 'Notebook', 'UNKNOWN']\n",
    "\n",
    "    print(\"\\nClassification report (including UNKNOWN):\\n\")\n",
    "    print(classification_report(\n",
    "        y_true_cls_valid,\n",
    "        final_preds_valid,\n",
    "        labels=[0, 1, 2, 3, 4, 5],\n",
    "        target_names=label_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"\\nClassification report (only real object classes):\\n\")\n",
    "    print(classification_report(\n",
    "        y_true_cls_valid,\n",
    "        final_preds_valid,\n",
    "        labels=[0, 1, 2, 3, 4],\n",
    "        target_names=label_names[:5],\n",
    "        zero_division=0\n",
    "    ))\n",
    "else:\n",
    "    print(\"Labels not available for this test set – skipping metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Model Predictions on the Test Set\n",
    "\n",
    "While quantitative metrics such as accuracy, precision/recall, and IoU provide measurable\n",
    "performance indicators, visual inspection is equally important for understanding how the model\n",
    "behaves on real test images. This section uses the `visualize_results` function to display:\n",
    "\n",
    "- The **input image**  \n",
    "- The **predicted bounding box** (in red)  \n",
    "- The **ground-truth bounding box** (in green), if labels are provided  \n",
    "- The **predicted class name**  \n",
    "- The **true class name**, highlighted when the prediction is incorrect  \n",
    "\n",
    "These visualizations help answer qualitative questions such as:\n",
    "\n",
    "- Does the model correctly focus on the object of interest?\n",
    "- Are the predicted bounding boxes tightly aligned with the object?\n",
    "- Does the model confuse certain object classes more than others?\n",
    "- Do low-confidence predictions correspond to ambiguous or difficult images?\n",
    "\n",
    "We can directly compare predicted and true boxes. If not, the visualization will still plot the predicted bounding boxes and class\n",
    "labels, which is useful for sanity-checking the model’s behavior.\n",
    "\n",
    "Visual inspection is particularly valuable for diagnosing failure cases and understanding class-specific\n",
    "patterns—for example, whether thin objects such as **Pens** or **Spoons** produce weaker localization\n",
    "performance compared to larger, more distinctive objects like **Mugs** and **Balls**.\n",
    "\n",
    "The following cell randomly selects a subset of test images and displays them side-by-side with their\n",
    "predictions for qualitative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(images, true_boxes, pred_boxes, pred_classes, true_classes=None, num_samples=5):\n",
    "    label_names = {0: 'Ball', 1: 'Mug', 2: 'Pen', 3: 'Spoon', 4: 'Notebook', 5: 'UNKNOWN'}\n",
    "    \n",
    "    total_samples = len(images)\n",
    "    num_samples = min(num_samples, total_samples)\n",
    "    indices = np.random.choice(total_samples, num_samples, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(3 * num_samples, 4))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = plt.subplot(1, num_samples, i + 1)\n",
    "        \n",
    "        img = images[idx]\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Ground truth box in green\n",
    "        if true_boxes is not None:\n",
    "            box_t = true_boxes[idx] * 100  # back to pixels\n",
    "            if box_t[2] > 0 and box_t[3] > 0:\n",
    "                rect_t = patches.Rectangle(\n",
    "                    (box_t[0], box_t[1]), box_t[2], box_t[3],\n",
    "                    linewidth=2, edgecolor='#00FF00', facecolor='none', label='Ground Truth'\n",
    "                )\n",
    "                ax.add_patch(rect_t)\n",
    "        \n",
    "        # Predicted box in red\n",
    "        box_p = pred_boxes[idx] * 100\n",
    "        if box_p[2] > 0 and box_p[3] > 0:\n",
    "            rect_p = patches.Rectangle(\n",
    "                (box_p[0], box_p[1]), box_p[2], box_p[3],\n",
    "                linewidth=2, edgecolor='#FF0000', facecolor='none', label='Prediction'\n",
    "            )\n",
    "            ax.add_patch(rect_p)\n",
    "            \n",
    "        pred_cls_idx = pred_classes[idx]\n",
    "        pred_name = label_names.get(pred_cls_idx, \"Unknown\")\n",
    "        \n",
    "        title_text = f\"Pred: {pred_name}\"\n",
    "        title_color = 'black'\n",
    "        \n",
    "        if true_classes is not None:\n",
    "            true_cls_idx = true_classes[idx]\n",
    "            if pred_cls_idx != true_cls_idx:\n",
    "                title_color = 'red'\n",
    "                true_name = label_names.get(true_cls_idx, \"Unknown\")\n",
    "                title_text += f\"\\n(True: {true_name})\"\n",
    "            else:\n",
    "                title_color = 'green'\n",
    "                \n",
    "        ax.set_title(title_text, color=title_color, fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Green = Ground Truth\\nRed   = Prediction\")\n",
    "\n",
    "visualize_results(\n",
    "    X_test[valid_mask],      # images with valid boxes\n",
    "    y_true_box[valid_mask],  # ground truth boxes\n",
    "    pred_boxes[valid_mask],  # predicted boxes\n",
    "    final_preds[valid_mask], # predicted classes\n",
    "    y_true_cls[valid_mask],  # true classes\n",
    "    num_samples=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8883027,
     "sourceId": 13938621,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
